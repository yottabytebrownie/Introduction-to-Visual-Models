# Week 2: Designing and Training Your Own CNN Architecture

##  Objectives

In this week, you will:

* Design and implement your own Convolutional Neural Network (CNN).
* Train CNNs using datasets like CIFAR-10 and CIFAR-100.
* Explore transfer learning using pretrained models.
* Understand and apply data augmentation techniques.
* Learn about famous architectures: Inception, ResNet, and VGG.



---

##  Topics Covered

### 1. **Designing Your Own CNN**

* Learn how to build a custom CNN model from scratch.
* Understand how to choose the number of layers, filter sizes, and pooling strategies.

 **Resource**:
 [Step-by-step Guide to Building a CNN (Medium)](https://medium.com/@sanjay_dutta/designing-your-own-convolutional-neural-network-cnn-model-a-step-by-step-guide-for-beginners-4e8b57836c81)

---

### 2. **Image Classification with CIFAR-10/CIFAR-100**

* Load and preprocess CIFAR datasets using TensorFlow.
* Train CNNs and evaluate performance.

 **Resource**:
 [Image Classification with CIFAR-10/100 (GeeksForGeeks)](https://www.geeksforgeeks.org/image-classification-using-cifar-10-and-cifar-100-dataset-in-tensorflow/)

---

### 3. **Transfer Learning**

* Use pretrained models like VGG, ResNet, or Inception.
* Fine-tune or use them as feature extractors for your dataset.

**Resource**:
 [transfer learning](https://www.tensorflow.org/tutorials/images/transfer_learning)

---

### 4. **Data Augmentation**

* Increase model robustness with real-time image transformations.
* Common techniques: flipping, cropping, rotation, zooming, etc.

**Resource**:
 [tensorflow link](https://www.tensorflow.org/tutorials/images/data_augmentation)

---

### 5. **Famous CNN Architectures**

* **VGG**: Deep but simple architecture with small filters.
* **ResNet**: Introduced skip connections to solve vanishing gradient.
* **Inception**: Parallel convolutions for multi-scale feature extraction.

---

### 6. **Activation Functions**

* Understand why **ReLU** is widely used.
* Learn when to use **Leaky ReLU** to avoid dying neurons and stagnant loss.

 **Watch**:
 Video explanation on why ReLU may cause the loss to not change — *([check this](https://youtu.be/Y-ruNSdpZ0Q?si=1Gm8_W9YWxcR7PjO))*

---

### 7. **Deep Learning Specialization: Andrew Ng (Videos 16–21)**

* Cover core CNN concepts:

  * Convolutions
  * Pooling
  * Padding
  * Fully Connected Layers
  * Transfer Learning

**Playlist**: [Andrew Ng Deep Learning Specialization](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0)
Focus on Videos 16–21.

---
Refer to [this](https://www.geeksforgeeks.org/loss-functions-in-deep-learning/)  to learn about loss functions andoptimizers.

Optional: Learn about image segmentation [U-net](https://www.jeremyjordan.me/semantic-segmentation/)








